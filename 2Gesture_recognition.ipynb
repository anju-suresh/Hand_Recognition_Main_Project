{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "YJr7_rjiAtNf",
   "metadata": {
    "id": "YJr7_rjiAtNf"
   },
   "source": [
    "# **Hand Gesture Recognition System for controlling Applications**\n",
    "\n",
    "The goal of this project is to train a Machine Learning algorithm capable of classifying images of different hand gestures, such as a 'thumbs up', 'palm' and control a media player to perform certain actions based on gestures. \n",
    "\n",
    "In this project, we are using MediaPipe framework, Neural Networks based on Tensorflow and Keras to train the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74961b9-2fdc-4fac-bdcb-adbdc67bbbb9",
   "metadata": {
    "id": "b74961b9-2fdc-4fac-bdcb-adbdc67bbbb9",
    "outputId": "a8a0a512-4632-49cf-8eab-e13ab6df12fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 00:18:21.767226: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages for hand gesture recognition project \n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import cv2\n",
    "import vlc\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qX3DgpMUF0IZ",
   "metadata": {
    "id": "qX3DgpMUF0IZ"
   },
   "source": [
    "### Function to capture frames from a webcam\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594ae213-258e-4b28-9b48-836418515a44",
   "metadata": {
    "id": "594ae213-258e-4b28-9b48-836418515a44"
   },
   "outputs": [],
   "source": [
    "def capture_frames(webcam):\n",
    "    _, frame = webcam.read()       # Read each frame from the webcam\n",
    "    frame = cv2.flip(frame, 1)     # Flip the frame vertically\n",
    "    cv2.imshow(\"Output\", frame)    # Show the final output\n",
    "    return frame\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tWAh8a8wGhge",
   "metadata": {
    "id": "tWAh8a8wGhge"
   },
   "source": [
    "### Function to detect hand keypoints(Landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1202246-0ead-4d7c-9d92-021b7c23f8ea",
   "metadata": {
    "id": "d1202246-0ead-4d7c-9d92-021b7c23f8ea"
   },
   "outputs": [],
   "source": [
    "def get_hand_landmarks(hands, frame):\n",
    "    x , y, c = frame.shape\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)      # Get hand landmark prediction\n",
    "  \n",
    "    if not result.multi_hand_landmarks:      # no hands detected     \n",
    "        return []\n",
    "    \n",
    "    # post process the result\n",
    "    landmarks = []\n",
    "    for handslms in result.multi_hand_landmarks:\n",
    "        for lm in handslms.landmark:\n",
    "            lmx = int(lm.x * x)\n",
    "            lmy = int(lm.y * y)\n",
    "            landmarks.append([lmx, lmy])\n",
    "            \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9cf4e5-c0ec-4230-acd5-15c3fd598227",
   "metadata": {
    "id": "6a9cf4e5-c0ec-4230-acd5-15c3fd598227"
   },
   "outputs": [],
   "source": [
    "#  def get_hand_crops_(hands, frame):\n",
    "    # import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shXulxqbo-2j",
   "metadata": {
    "id": "shXulxqbo-2j"
   },
   "source": [
    "### Function to get hand landmark prediction and detect hand frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd58ffab-a4a9-4208-b91b-bc4824f4e723",
   "metadata": {
    "id": "dd58ffab-a4a9-4208-b91b-bc4824f4e723"
   },
   "outputs": [],
   "source": [
    "def get_hand_crops(hands, frame):\n",
    "    height , width, c = frame.shape\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Get hand landmark prediction\n",
    "    result = hands.process(framergb)\n",
    "    \n",
    "    if not result.multi_hand_landmarks:\n",
    "        return []  # no hands detected\n",
    "    \n",
    "    # post process the result\n",
    "            \n",
    "    for hand_landmark in result.multi_hand_landmarks:\n",
    "        \n",
    "        xList = [landmark.x*width for landmark in hand_landmark.landmark]\n",
    "        yList = [landmark.y*height for landmark in hand_landmark.landmark]\n",
    "        \n",
    "        xmin, xmax = int(min(xList)), int(max(xList))\n",
    "        ymin, ymax = int(min(yList)), int(max(yList))\n",
    "        bbox = [xmin, ymin, xmax, ymax]\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        \n",
    "        bboxInfo = {\"bbox\": bbox}  \n",
    "        delta = 0\n",
    "        crop = frame[y1-delta:y2+delta, x1-delta:x2+delta].copy()       \n",
    "       \n",
    "        return crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b36148-a065-46a5-bf6e-bda3ad143eff",
   "metadata": {
    "id": "71b36148-a065-46a5-bf6e-bda3ad143eff"
   },
   "outputs": [],
   "source": [
    " # def identify_gesture(model, landmarks, classNames):\n",
    "#     # Predict gesture\n",
    "#     prediction = model.predict([landmarks])\n",
    "#     # print(prediction)\n",
    "#     classID = np.argmax(prediction)\n",
    "#     className = classNames[classID]\n",
    "#      return className\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "osyxIz8-mp7y",
   "metadata": {
    "id": "osyxIz8-mp7y"
   },
   "source": [
    "### Pre-processing croped hand image for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9debb0-6540-4d45-9b4c-467b70b7fafa",
   "metadata": {
    "id": "fb9debb0-6540-4d45-9b4c-467b70b7fafa"
   },
   "outputs": [],
   "source": [
    "def image_preprocess(image):\n",
    "    hand_crop_resize = cv2.resize(image, (224, 224))\n",
    "    hand_cropGRY = cv2.cvtColor(hand_crop_resize, cv2.COLOR_BGR2GRAY)\n",
    "    hand_cropGRY_3C = cv2.cvtColor(hand_cropGRY, cv2.COLOR_GRAY2RGB)\n",
    "    hand_cropGRY_3C = hand_cropGRY_3C/255\n",
    "    return hand_cropGRY_3C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dkDC3qQ5mMQB",
   "metadata": {
    "id": "dkDC3qQ5mMQB"
   },
   "source": [
    "### Function to predict gesture from croped hand image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1968e3d0-b113-44c4-83d8-9ca4b83ca083",
   "metadata": {
    "id": "1968e3d0-b113-44c4-83d8-9ca4b83ca083"
   },
   "outputs": [],
   "source": [
    "def get_gesture(model, hand_crop, classNames):\n",
    "    image = image_preprocess(hand_crop)\n",
    "    prediction = model.predict(image.reshape(1,224,224,3)) \n",
    "    classID = np.argmax(prediction) #find classID\n",
    "    gesture_name = classNames[classID] #find hand gesture\n",
    "    return gesture_name\n",
    "                               \n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hX5zgwbxlqM2",
   "metadata": {
    "id": "hX5zgwbxlqM2"
   },
   "source": [
    "### Function to control media player operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c925124-c69d-40d2-b8bc-c79d5ea970ee",
   "metadata": {
    "id": "5c925124-c69d-40d2-b8bc-c79d5ea970ee"
   },
   "outputs": [],
   "source": [
    "def invoke_music_controls(media_player, class_name):\n",
    "    \n",
    "    print(f\"{class_name} invoked\") #print gesture name\n",
    "    if class_name == \"play\":\n",
    "        \n",
    "        media_player.play()\n",
    "    \n",
    "    if class_name == \"stop\":\n",
    "       \n",
    "        media_player.stop()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fbq1yCJKcqwQ",
   "metadata": {
    "id": "Fbq1yCJKcqwQ"
   },
   "source": [
    "### Function to Invoke Media player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf2e4c77-671c-4c43-8bcc-5106359eb543",
   "metadata": {
    "id": "bf2e4c77-671c-4c43-8bcc-5106359eb543"
   },
   "outputs": [],
   "source": [
    "def get_media_player(player_name=\"vlc\"):\n",
    "    player = vlc.MediaPlayer(\"StarWars60.wav\")\n",
    "    return player"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hVDdKb_WSFQB",
   "metadata": {
    "id": "hVDdKb_WSFQB"
   },
   "source": [
    "### An Event loop that monitor user inputs continously, This function performs following operations:\n",
    "\n",
    "\n",
    "*  Capture frames upon input from User\n",
    "\n",
    "*  Get hand landmarks from the captured frames\n",
    "\n",
    "*  Using lanmarks, crop the region where hand is detected\n",
    "*  Predict the gesture using croped hand image\n",
    "\n",
    "\n",
    "*  Use predicted result to invoke music player\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c70f2f80-495f-4077-8ecb-2b27d5927748",
   "metadata": {
    "id": "c70f2f80-495f-4077-8ecb-2b27d5927748"
   },
   "outputs": [],
   "source": [
    "def run_loop(media_player, webcam, hands, model, class_names, delay=0.5):\n",
    "    while True:\n",
    "        gesture_name = None\n",
    "        landmarks = None\n",
    "        crop = None\n",
    "        \n",
    "        # Read each frame from the webcam\n",
    "        frames = capture_frames(webcam)\n",
    "        landmarks = get_hand_landmarks(hands, frames)\n",
    "        \n",
    "        if landmarks:\n",
    "            crop = get_hand_crops(hands, frames)\n",
    "            cv2.imshow(\"icrop\", image_preprocess(crop))\n",
    "            \n",
    "        if crop is not None:\n",
    "            gesture_name = get_gesture(model, crop, class_names)\n",
    "        \n",
    "            \n",
    "        if gesture_name:\n",
    "            invoke_music_controls(media_player, gesture_name)\n",
    "    \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "                break\n",
    "\n",
    "        time.sleep(delay)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cd64-fHqH8gW",
   "metadata": {
    "id": "Cd64-fHqH8gW"
   },
   "source": [
    "### This function will do following operations:\n",
    "\n",
    "\n",
    "\n",
    "*   Initialize Mediapipe\n",
    "*   Load Neural Network Model for Gesture Recognition\n",
    "*   Load the file for Gesture Classification\n",
    "*   Initialize Webcam\n",
    "*   Media Player operations based on hand gestures\n",
    "*   Finally release webcam and close all windows after operations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22f68845-4457-49ab-bcc0-39719bcf04ab",
   "metadata": {
    "id": "22f68845-4457-49ab-bcc0-39719bcf04ab"
   },
   "outputs": [],
   "source": [
    "def monitor_for_hand_gesture():\n",
    "\n",
    "    # initialize MediaPipe\n",
    "    mpHands = mp.solutions.hands\n",
    "    hands = mpHands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "    mpDraw = mp.solutions.drawing_utils\n",
    "    hand_crop = None\n",
    "    \n",
    "    # Load Hand Gesture Recognizer model\n",
    "    h_model = load_model('model_hand_gesture.h5')\n",
    "\n",
    "    # Load Class Names(Gesture Names)\n",
    "    f = open('gesture.names', 'r')\n",
    "    class_names = [l.strip() for l in f.readlines()]\n",
    "    f.close()\n",
    "    \n",
    "    # Initialize the Webcam\n",
    "    webcam = cv2.VideoCapture(0)\n",
    "    \n",
    "    # get Media player\n",
    "    media_player = get_media_player()\n",
    "\n",
    "    try:\n",
    "        #Event loop that monitor user inputs continously\n",
    "        run_loop(media_player, webcam, hands, h_model, class_names)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        # release the webcam and destroy all active windows\n",
    "        webcam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        #cv2.waitKey(1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a958e85e-4261-47c6-851f-44429ae233fa",
   "metadata": {
    "id": "a958e85e-4261-47c6-851f-44429ae233fa",
    "outputId": "2957db15-6a1f-4828-8e09-99a406a15003"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2023-01-15 00:18:29.308517: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "play invoked\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "play invoked\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "play invoked\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "stop invoked\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "stop invoked\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "play invoked\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "play invoked\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "stop invoked\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "stop invoked\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "stop invoked\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "play invoked\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "play invoked\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "stop invoked\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "stop invoked\n"
     ]
    }
   ],
   "source": [
    "monitor_for_hand_gesture()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0185b84-041e-4e43-933d-6770d69dae0f",
   "metadata": {
    "id": "a0185b84-041e-4e43-933d-6770d69dae0f"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
